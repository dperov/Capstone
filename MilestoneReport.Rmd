---
title: "Capstone Milestone Report"
author: "Dmitri Perov"
date: "March 20, 2016"
output: html_document
---


The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs (http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. 

The motivation for this project is to: 

1. Demonstrate that you've downloaded the data and have successfully loaded it in.

2. Create a basic report of summary statistics about the data sets.

3. Report any interesting findings that you amassed so far.

4. Get feedback on your plans for creating a prediction algorithm and Shiny app.

Data Load

Data Tokenization

Profanity Filtering

### Introduction

### Dataset 

Dataset is downloaded from Coursera site. Zipped file is 561M in size. Unzipped content is 1.5G in size.
Text corpus contains tests in different languages. We will use English files for our experiments.
These files are

* en_US.blogs.txt
* en_US.news.txt
* en_US.twitter.txt

Its sizes in lines are

```{r, echo=TRUE,eval=FALSE}
library (R.utils)
countLines("Coursera-SwiftKey/final/en_US/en_US.blogs.txt")
countLines("Coursera-SwiftKey/final/en_US/en_US.news.txt")
countLines("Coursera-SwiftKey/final/en_US/en_US.twitter.txt")
```
```{r,eval=FALSE}
[1] 899288
[1] 1010242
[1] 2360148

```
#### Loading Data Files

We will use R 'tm' for text loading and processing. This package allows to create im-memory database called "VCorpus" which allows wide range of text 
processing apporaches.

```{r,eval=FALSE}
corpus <- VCorpus(DirSource("Coursera-SwiftKey/final/en_US/", encoding = "UTF-8"), readerControl = list(reader = readPlain))
corpus
```
```{r,eval=FALSE}
<<VCorpus>>
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 3
```


#### Corpus preprocessing

tm package allows us very easily make all required text preprocessing, i.e. 
* removing whitespace
* convering to lowercase
* profanity filtering
* punctuation and nubmers removal

Profanity list can be downloaded from https://gist.github.com/tjrobinson/2366772

```{r,eval=FALSE}
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
tb <- read.csv("Terms-to-Block.csv")
tb2 <- as.vector(tb[4:726, 2])
tb3 <- gsub(',', '', tb2)
corpus <- tm_map(corpus, removeWords, tb3)
corpus <- tm_map(corpus, removeWords, profwords)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)  
```

#### Corpus cleaning up

Then we create Document Term Matrix

```{r,eval=FALSE}
dtm <- DocumentTermMatrix(corpus)   
dtm
```

Inspecting matrix via `inspect(dtm[1:5, 1:20])` we found that there are many
misspelled word, punctuation, non-printable chareaters. tm package has funstionality for removing low frequency term.
Functon `removeSparseTerms` require settin sparsity coefficieng whcih has to be selected manually. We found that value 0.1 works well.

```{r,eval=FALSE}
dtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.   
inspect(dtms)  
```

Following histograms shows top most frequently words.


### Corpus N-Gram processing



